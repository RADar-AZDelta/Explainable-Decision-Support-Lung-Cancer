{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the correct version of the required packages (in case you haven't done it yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r '../requirements.txt'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import tqdm\n",
    "import joblib\n",
    "import yaml\n",
    "\n",
    "# Preprocessing\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder,RobustScaler\n",
    "\n",
    "\n",
    "# Models\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    VotingClassifier\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Parameter tuning, splitting\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RepeatedStratifiedKFold,\n",
    "    cross_val_score,\n",
    "    train_test_split,\n",
    ")\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/dataset_for_training.csv\", header=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_models():\n",
    "    models = dict()\n",
    "    models[\"KNN\"] = KNeighborsClassifier()\n",
    "    models[\"LOR\"] = LogisticRegression()\n",
    "    models[\"SVM\"] = SVC(probability=True)\n",
    "    models[\"RF\"] = RandomForestClassifier()\n",
    "    models[\"XGB\"] = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
    "    return models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline(model_abbreviation, model, categorical_indices, continuous_indices):\n",
    "    scaler = RobustScaler()\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "    scalingsteps = [(\"cont_imputer\", imputer), (\"continuous\", scaler)]\n",
    "\n",
    "    sampler = SMOTE(random_state=42)\n",
    "\n",
    "\n",
    "    scaling_pipeline = Pipeline(steps=scalingsteps)\n",
    "    encoding_pipeline = Pipeline(\n",
    "                steps=[\n",
    "                    (\n",
    "                        \"categorical\",\n",
    "                        OneHotEncoder(handle_unknown=\"ignore\", drop=\"if_binary\"),\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "    if model_abbreviation == \"ensemble\":\n",
    "        finalmodel = model\n",
    "    else:\n",
    "        finalmodel = CalibratedClassifierCV(\n",
    "            base_estimator=model, cv=5, ensemble=True\n",
    "        )\n",
    "    preprocess = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"scal\", scaling_pipeline, continuous_indices),\n",
    "                (\"cat\", encoding_pipeline, categorical_indices),\n",
    "            ],\n",
    "            remainder=\"passthrough\",\n",
    "        )\n",
    "    pipeline = Pipeline(\n",
    "            steps=[\n",
    "                (\"preprocess\", preprocess),\n",
    "                (\"sampler\", sampler),\n",
    "                (\"model\", finalmodel),\n",
    "            ])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def gridsearch(pipeline, abbrev, outcome, X_train, y_train):\n",
    "    \"\"\"Gridsearch to find optimal parameters for a ML algorithm based on ROC AUC score.\n",
    "    Saves:\n",
    "    Yaml file with the optimal parameters in the model's outcomefolder.\"\"\"\n",
    "    file = \"../parameters/gridsearch.yaml\"\n",
    "    dict = yaml.safe_load(open(file, \"rb\"))\n",
    "    \n",
    "    if abbrev == \"RF\":\n",
    "        param_grid = dict[\"rf_param\"]\n",
    "        param_grid[\"model\"] = [RandomForestClassifier()]\n",
    "    elif abbrev == \"SVM\":\n",
    "        param_grid = dict[\"svm_param\"]\n",
    "        param_grid[\"model\"] = [SVC()]\n",
    "    elif abbrev == \"KNN\":\n",
    "        param_grid = dict[\"knn_param\"]\n",
    "        param_grid[\"model\"] = [KNeighborsClassifier()]\n",
    "    elif abbrev == \"LOR\":\n",
    "        param_grid = dict[\"lor_param\"]\n",
    "        param_grid[\"model\"] = [LogisticRegression()]\n",
    "    else:\n",
    "        param_grid = dict[\"xgb_param\"]\n",
    "        param_grid[\"model\"] = [XGBClassifier()]\n",
    "    grid = GridSearchCV(\n",
    "        pipeline, param_grid, scoring='roc_auc', cv=5, verbose=1, n_jobs=-1\n",
    "    )\n",
    "    grid.fit(X_train, y_train)\n",
    "    grid_params = grid.best_params_\n",
    "    outputpath = os.path.join(f\"../parameters/{outcome}/{abbrev}\")\n",
    "    os.makedirs(outputpath, exist_ok=True)\n",
    "    with open(os.path.join(outputpath, \"bestparameters.yaml\"), \"w\") as outfile:\n",
    "        yaml.dump(grid_params, outfile, default_flow_style=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"LOR\", \"KNN\", \"SVM\", \"RF\", \"XGB\"]\n",
    "classificationtype = \"binary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_abbreviation in models:\n",
    "    modeldict = get_classification_models()\n",
    "    y = df[\"y\"]\n",
    "    X = df.drop([\"y\"], axis=1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.20,stratify=y)\n",
    "    categorical_features = list(X.select_dtypes(include=[\"object\"]))\n",
    "    categorical_indices = [X.columns.get_loc(cat) for cat in categorical_features]\n",
    "    continuous_features = list(X.select_dtypes(include=[\"float\", \"int\"]))\n",
    "    continuous_indices = [X.columns.get_loc(cont) for cont in continuous_features]\n",
    "    pipeline = create_pipeline(model_abbreviation=model_abbreviation, model=modeldict[model_abbreviation], categorical_indices=categorical_indices, continuous_indices=continuous_indices)\n",
    "    gridsearch(pipeline, model_abbreviation, \"M_6weeks\", X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_models_with_parameters(outcome):\n",
    "    modeldict = dict()\n",
    "    with open(f'../parameters/{outcome}/KNN/bestparameters.yaml') as knnfile:\n",
    "        knn = yaml.load(knnfile, Loader=yaml.Loader)\n",
    "    with open(f'../parameters/{outcome}/LOR/bestparameters.yaml') as lorfile:\n",
    "        lor = yaml.load(lorfile, Loader=yaml.Loader)\n",
    "    with open(f'../parameters/{outcome}/SVM/bestparameters.yaml') as svmfile:\n",
    "        svm = yaml.load(svmfile, Loader=yaml.Loader)\n",
    "    with open(f'../parameters/{outcome}/RF/bestparameters.yaml') as rffile:\n",
    "        rf = yaml.load(rffile, Loader=yaml.Loader)\n",
    "    with open(f'../parameters/{outcome}/XGB/bestparameters.yaml') as xgbfile:\n",
    "        xgb = yaml.load(xgbfile, Loader=yaml.Loader)\n",
    "\n",
    "    modeldict[\"KNN\"] = KNeighborsClassifier(\n",
    "            n_neighbors=knn[\"model__n_neighbors\"],\n",
    "            weights=knn[\"model__weights\"],\n",
    "            metric=knn[\"model__metric\"],\n",
    "        )\n",
    "    modeldict[\"LOR\"] = LogisticRegression(\n",
    "            solver=lor[\"model__solver\"],\n",
    "            C=lor[\"model__C\"],\n",
    "            max_iter=lor[\"model__max_iter\"],\n",
    "            penalty=lor[\"model__penalty\"],\n",
    "            random_state=42,\n",
    "        )\n",
    "    modeldict[\"SVM\"] = SVC(\n",
    "            C=svm[\"model__C\"],\n",
    "            gamma=svm[\"model__gamma\"],\n",
    "            kernel=\"rbf\",\n",
    "            probability=True,\n",
    "            random_state=42,\n",
    "        )\n",
    "    modeldict[\"RF\"] = RandomForestClassifier(\n",
    "            min_samples_split=rf[\"model__min_samples_split\"],\n",
    "            n_estimators=rf[\"model__n_estimators\"],\n",
    "            max_depth=rf[\"model__max_depth\"],\n",
    "            max_features=\"sqrt\",\n",
    "            min_samples_leaf=rf[\"model__min_samples_leaf\"],\n",
    "            random_state=42,\n",
    "        )\n",
    "    modeldict[\"XGB\"] = XGBClassifier(\n",
    "            learning_rate=xgb[\"model__learning_rate\"],\n",
    "            max_depth=xgb[\"model__max_depth\"],\n",
    "            n_estimators=xgb[\"model__n_estimators\"],\n",
    "            min_child_weight=xgb[\"model__min_child_weight\"],\n",
    "            subsample=xgb[\"model__subsample\"],\n",
    "            eval_metric=xgb[\"model__eval_metric\"],\n",
    "            random_state=42,\n",
    "        )\n",
    "    return modeldict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(pipeline, X_train, y_train, classificationtype,features):\n",
    "    pipeline.fit(X_train, np.ravel(y_train))\n",
    "    y_hat_train = pipeline.predict(X_train)\n",
    "    y_prob_train = pipeline.predict_proba(X_train)\n",
    "    train_acc = accuracy_score(y_train, y_hat_train)\n",
    "    if classificationtype == \"binary\":\n",
    "        train_auc = roc_auc_score(y_train, y_prob_train[:, 1])\n",
    "    else:\n",
    "        train_auc = roc_auc_score(\n",
    "            y_train, y_prob_train, multi_class=\"ovo\", average=\"macro\"\n",
    "        )\n",
    "    report = classification_report(y_train, y_hat_train)\n",
    "    print(\"Train results of {} with {} features\".format(pipeline.named_steps[\"model\"], len(features)))\n",
    "    print(\"Train accuracy: {:.2f}\".format(train_acc))\n",
    "    print(\"Train AUC: {:.2f}\".format(train_auc))\n",
    "    print(report)\n",
    "    return {\"train_acc\": train_acc, \"train_auc\": train_auc, \"train_report\": report}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(pipeline, X_train, y_train, X_test, y_test, classificationtype,features):\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_hat = pipeline.predict(X_test)\n",
    "    y_prob = pipeline.predict_proba(X_test)\n",
    "    test_acc = accuracy_score(y_test, y_hat)\n",
    "    if classificationtype == \"binary\":\n",
    "        test_auc = roc_auc_score(y_test, y_prob[:, 1], average=\"macro\")\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_hat).ravel()\n",
    "    else:\n",
    "        test_auc = roc_auc_score(\n",
    "            y_test, y_prob, multi_class=\"ovr\", average=\"macro\"\n",
    "        )\n",
    "        tn, fp, fn, tp = \"nvt\", \"nvt\", \"nvt\", \"nvt\"\n",
    "    F1 = f1_score(y_test, y_hat, average=\"macro\")\n",
    "    precision = precision_score(y_test, y_hat, average=\"macro\")\n",
    "    recall = recall_score(y_test, y_hat, average=\"macro\")\n",
    "    print(\"Test results of {} with {} features\".format(pipeline.named_steps[\"model\"], len(features)))\n",
    "    print(\"Test accuracy: {:.2f}\".format(test_acc))\n",
    "    print(\"Test AUC: {:.2f}\".format(test_auc))\n",
    "    print(\"Test F1-score: {:.2f}\".format(F1))\n",
    "    report = classification_report(y_test, y_hat)\n",
    "    print(report)\n",
    "    return {\n",
    "        \"test_acc\": test_acc,\n",
    "        \"test_auc\": test_auc,\n",
    "        \"test_report\": report,\n",
    "        \"tn\": tn,\n",
    "        \"fp\": fp,\n",
    "        \"fn\": fn,\n",
    "        \"tp\": tp,\n",
    "        \"test_F1\": F1,\n",
    "        \"test_recall\": recall,\n",
    "        \"test_precision\": precision,\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classification_ensemble_model(estimators, X_train, y_train, X_test, y_test, classificationtype,features, categorical_indices, continuous_indices):\n",
    "    base_estimators = list()  # base models\n",
    "    weights = list()\n",
    "    results = dict()\n",
    "    counter = 0\n",
    "    models = get_classification_models_with_parameters()\n",
    "    for abbreviation in estimators:\n",
    "        ML_model = models[abbreviation]\n",
    "        pipeline = create_pipeline(abbreviation,ML_model, categorical_indices, continuous_indices)\n",
    "        train_results = train(pipeline, X_train, y_train, classificationtype,features)\n",
    "        test_results = test(pipeline, X_train, y_train, X_test, y_test, classificationtype,features)\n",
    "        weights.append(test_results[\"test_auc\"] * test_results[\"test_auc\"])\n",
    "        results[abbreviation] = {\n",
    "            \"ACC\": test_results[\"test_acc\"],\n",
    "            \"AUC\": test_results[\"test_auc\"],\n",
    "            \"RECALL\": test_results[\"test_recall\"],\n",
    "            \"PRECISION\": test_results[\"test_precision\"],\n",
    "            \"F1\": test_results[\"test_F1\"],\n",
    "        }\n",
    "        base_estimators.append(\n",
    "            (\n",
    "                f\"{abbreviation}_{counter}\",\n",
    "                Pipeline(\n",
    "                    steps=[\n",
    "                        (\n",
    "                            f\"{abbreviation}_{counter}\",\n",
    "                            pipeline.named_steps[\"model\"],\n",
    "                        )\n",
    "                    ]\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "        counter += 1\n",
    "    normalized_weights = [float(i / sum(weights)) for i in weights]\n",
    "    ensemble = VotingClassifier(\n",
    "        base_estimators, voting=\"soft\", weights=normalized_weights\n",
    "    )\n",
    "    return ensemble, results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_interval_classification(all_accuracy, all_aucs, all_recall, all_precision, all_f1):\n",
    "    all_accuracy.sort()\n",
    "    all_aucs.sort()\n",
    "    all_recall.sort()\n",
    "    all_precision.sort()\n",
    "    all_f1.sort()\n",
    "\n",
    "    print(f\"Confidence interval accuracy: {np.array(all_accuracy).mean()} [{np.array(all_accuracy).mean() - (2*np.array(all_accuracy).std())}-{np.array(all_accuracy).mean()+(2*np.array(all_accuracy).std())}]\\n\")\n",
    "    print(f\"Confidence interval AUC: {np.array(all_aucs).mean()} [{np.array(all_aucs).mean() - (2*np.array(all_aucs).std())}-{np.array(all_aucs).mean()+(2*np.array(all_aucs).std())}]\\n\")\n",
    "    print(f\"Confidence interval recall: {np.array(all_recall).mean()} [{np.array(all_recall).mean() - (2*np.array(all_recall).std())}-{np.array(all_recall).mean()+(2*np.array(all_recall).std())}]\\n\")\n",
    "    print(f\"Confidence interval precision: {np.array(all_precision).mean()} [{np.array(all_precision).mean() - (2*np.array(all_precision).std())}-{np.array(all_precision).mean()+(2*np.array(all_precision).std())}]\\n\")\n",
    "    print(f\"Confidence interval F1-score: {np.array(all_f1).mean()} [{np.array(all_f1).mean() - (2*np.array(all_f1).std())}-{np.array(all_f1).mean()+(2*np.array(all_f1).std())}]\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pipeline(pipeline, model_abbreviation, features, train_results, test_results, split):\n",
    "    joblib.dump(pipeline, f\"../results/{model_abbreviation}_bestpipeline.pkl\")\n",
    "    with open(f'../results/{model_abbreviation}_bestpipeline_results.txt', 'w') as file:\n",
    "        file.write(\"Data was split with random_state={}\\n\".format(split))\n",
    "        file.write(\"Train accuracy: {:.2f}\\n\".format(train_results[\"train_acc\"]))\n",
    "        file.write(\"Train AUC of {:.2f}\\n\".format(train_results[\"train_auc\"]))\n",
    "        file.write(\"{}\\n\".format(train_results[\"train_report\"]))\n",
    "        file.write(\"Test accuracy: {:.2f}\\n\".format(test_results[\"test_acc\"]))\n",
    "        file.write(\"Test AUC of {:.2f}\\n\".format(test_results[\"test_auc\"]))\n",
    "        file.write(\"TN: {}\\tFP: {}\\tFN: {}\\tTP: {}\\n\".format(test_results[\"tn\"], test_results[\"fp\"], test_results[\"fn\"], test_results[\"tp\"]))\n",
    "        file.write(\"{}\\n\\n\".format(test_results[\"test_report\"]))\n",
    "        file.write(\"Features:\\n\")\n",
    "        for feat in features:\n",
    "            file.write(f\"{feat}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_loop(dataset, model_abbreviation, outcome=\"M_6weeks\", loops=1, save=False):\n",
    "    best_auc = 0\n",
    "    all_aucs = []\n",
    "    all_recall = []\n",
    "    all_precision = []\n",
    "    all_f1 = []\n",
    "    all_accuracies = []\n",
    "    for i in tqdm.tqdm(range(0,int(loops))):\n",
    "        y = dataset[\"y\"]\n",
    "        X = dataset.drop([\"y\"], axis=1)\n",
    "        categorical_features = list(X.select_dtypes(include=[\"object\"]))\n",
    "        categorical_indices = [X.columns.get_loc(cat) for cat in categorical_features]\n",
    "        continuous_features = list(X.select_dtypes(include=[\"float\", \"int\"]))\n",
    "        continuous_indices = [X.columns.get_loc(cont) for cont in continuous_features]\n",
    "        features = X.columns\n",
    "        # Split dataset\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.20,stratify=y,random_state=i)\n",
    "        print(\"Training set: {}, Test set: {}\".format(len(X_train), len(X_test)))\n",
    "\n",
    "        if model_abbreviation == \"ensemble\":\n",
    "            voting_classifier, results_seperate_estimators = create_classification_ensemble_model(models, X_train, y_train, X_test, y_test, classificationtype,features,  categorical_indices, continuous_indices)\n",
    "            pipeline = create_pipeline(model_abbreviation, voting_classifier, categorical_indices, continuous_indices)\n",
    "           \n",
    "        else:\n",
    "            modeldict = get_classification_models_with_parameters(outcome)\n",
    "            model = modeldict[model_abbreviation]\n",
    "            pipeline = create_pipeline(model_abbreviation, model, categorical_indices, continuous_indices)\n",
    "        train_results = train(pipeline, X_train, y_train, classificationtype,features)\n",
    "        test_results = test(pipeline, X_train, y_train, X_test, y_test, classificationtype,features)\n",
    "        all_accuracies.append(test_results[\"test_acc\"])\n",
    "        all_aucs.append(test_results[\"test_auc\"])\n",
    "        all_recall.append(test_results[\"test_recall\"])\n",
    "        all_precision.append(test_results[\"test_precision\"])\n",
    "        all_f1.append(test_results[\"test_F1\"])\n",
    "        \n",
    "        if save:\n",
    "            if test_results[\"test_auc\"] > best_auc:\n",
    "                save_pipeline(pipeline, model_abbreviation, features, train_results, test_results, i)\n",
    "                best_auc = test_results[\"test_auc\"]\n",
    "    if loops > 20:\n",
    "        confidence_interval_classification(all_accuracies,all_aucs,all_recall,all_precision,all_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_loop(df, \"RF\", \"M_6weeks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_loop(df, \"ensemble\", loops=20, save=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
